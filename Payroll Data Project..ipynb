{
  "metadata": {
    "name": "Payroll Data Project",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "var df \u003d spark.read.format(\"csv\").option(\"header\",\"true\").load(\"/YourMoney_Agency_Payroll.csv\")"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\ndf.show(1000, true)"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "df.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n//Replace - for null on  Middlename column \n\nval middle_name_null \u003ddf.withColumn(\"MIDDLE_INITIAL\",\n        when(col(\"MIDDLE_INITIAL\").equalTo(\"\"), lit(\"none\"))\n                .otherwise(col(\"MIDDLE_INITIAL\"))\n)\n// count()"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "//Only update nulls in salary_hourly_rate where record_type is detail one                \r\nval salary_hourly_rate_null \u003d middle_name_null.withColumn(\"SALARY_HOURLY_RATE\",\r\n  when(col(\"SALARY_HOURLY_RATE\").isNull \u0026\u0026 col(\"RECORD_TYPE\") \u003d\u003d\u003d \"MASTER\", lit(0))\r\n    .otherwise(col(\"SALARY_HOURLY_RATE\"))\r\n)\r\nval master_ytd_regular_pay_null \u003d salary_hourly_rate_null.withColumn(\"MASTER_YTD_REGULAR_PAY\",\r\n  when(col(\"MASTER_YTD_REGULAR_PAY\").isNull \u0026\u0026 col(\"RECORD_TYPE\") \u003d\u003d\u003d \"MASTER\", lit(0))\r\n    .otherwise(col(\"MASTER_YTD_REGULAR_PAY\"))\r\n)\r\n\r\nval master_ytd_overtime_payments_null \u003d master_ytd_regular_pay_null.withColumn(\"MASTER_YTD_OVERTIME_PAYMENTS\",\r\n  when(col(\"MASTER_YTD_OVERTIME_PAYMENTS\").isNull \u0026\u0026 col(\"RECORD_TYPE\") \u003d\u003d\u003d \"MASTER\", lit(0))\r\n    .otherwise(col(\"MASTER_YTD_OVERTIME_PAYMENTS\"))\r\n)\r\n\r\nval master_ytd_all_other_payments_null \u003d master_ytd_overtime_payments_null.withColumn(\"MASTER_YTD_ALL_OTHER_PAYMENTS\",\r\n  when(col(\"MASTER_YTD_ALL_OTHER_PAYMENTS\").isNull \u0026\u0026 col(\"RECORD_TYPE\") \u003d\u003d\u003d \"MASTER\", lit(0))\r\n    .otherwise(col(\"MASTER_YTD_ALL_OTHER_PAYMENTS\"))\r\n)\r\n\r\nval master_ytd_earnings_null \u003d master_ytd_all_other_payments_null.withColumn(\"MASTER_YTD_EARNINGS\",\r\n  when(col(\"MASTER_YTD_EARNINGS\").isNull \u0026\u0026 col(\"RECORD_TYPE\") \u003d\u003d\u003d \"MASTER\", lit(0))\r\n    .otherwise(col(\"MASTER_YTD_EARNINGS\"))\r\n)\r\n\r\nval regular_pay_null \u003d master_ytd_earnings_null.withColumn(\"REGULAR_PAY\",\r\n  when(col(\"REGULAR_PAY\").isNull \u0026\u0026 col(\"RECORD_TYPE\") \u003d\u003d\u003d \"DETAIL\", lit(0))\r\n    .otherwise(col(\"REGULAR_PAY\"))\r\n)\r\n\r\nval supplemental_pay_null \u003d regular_pay_null.withColumn(\"SUPPLEMENTAL_PAY\",\r\n  when(col(\"SUPPLEMENTAL_PAY\").isNull \u0026\u0026 col(\"RECORD_TYPE\") \u003d\u003d\u003d \"DETAIL\", lit(0))\r\n    .otherwise(col(\"SUPPLEMENTAL_PAY\"))\r\n)\r\n\r\n\r\n\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Save the cleaned data to a Hive table\r\nsupplemental_pay_null.write.saveAsTable(\"payroll_data_cleaned\")"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Calculate total earnings\r\nval total_earnings \u003d supplemental_pay_null.withColumn(\"TOTAL_EARNINGS\", col(\"REGULAR_PAY\") + col(\"SUPPLEMENTAL_PAY\") + col(\"ONE_TIME_PAYMENTS\"))\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "//Group by calendar year and calculate the average total earnings\r\n\r\nimport org.apache.spark.sql.functions._\r\n\r\n// Group by calendar year and calculate the average total earnings\r\nval avg_earnings_by_year \u003d total_earnings.groupBy(\"CALENDAR_YEAR\").agg(avg(\"TOTAL_EARNINGS\").alias(\"AVG_TOTAL_EARNINGS\"))\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Apply a filter to include only records with salary/hourly rate greater than a certain threshold\r\nval filtered_data \u003d supplemental_pay_null.filter(col(\"SALARY_HOURLY_RATE\") \u003e 20)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Show the filtered data\r\nfiltered_data.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Calculate the average overtime payments per employee\r\nval average_overtime_payments \u003d supplemental_pay_null.groupBy(\"PAYROLL_ID\").agg(avg(\"OVERTIME_PAYMENTS\").alias(\"AVG_OVERTIME_PAYMENTS\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Identify employees with consistently high overtime payments\r\nval high_overtime_employees \u003d average_overtime_payments.filter(col(\"AVG_OVERTIME_PAYMENTS\") \u003e 1000)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Calculate turnover rates per department\r\nval turnover_rates \u003d supplemental_pay_null.groupBy(\"MASTER_DEPARTMENT_AGENCY_DESC\").agg((countDistinct(\"PAYROLL_ID\") / countDistinct(\"PAYROLL_ID\")).alias(\"TURNOVER_RATE\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Visualize employee earnings\r\ntotal_earnings.createOrReplaceTempView(\"payroll_data\")\r\nval earnings_by_year \u003d spark.sql(\"SELECT CALENDAR_YEAR, SUM(TOTAL_EARNINGS) AS TOTAL_EARNINGS FROM payroll_data GROUP BY CALENDAR_YEAR\")\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\r\n-- Visualize total earnings by year\r\nSELECT CALENDAR_YEAR, SUM(TOTAL_EARNINGS) AS TOTAL_EARNINGS\r\nFROM payroll_data\r\nGROUP BY CALENDAR_YEAR"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\r\nval df_cleaned \u003d spark.table(\"payroll_data\")\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\r\nimport pandas as pd\r\n\r\ndf_pandas \u003d df_cleaned.toPandas()\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    }
  ]
}